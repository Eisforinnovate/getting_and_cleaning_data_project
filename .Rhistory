}
add2(3,5)
}
add2 <- function(x,y){
x+y
}
add2(3.5)
add2(3,4)
above <-function(x,n){
use <- x>n
x[use]
}
x<-1:20
above(x,12)
columnmean<- function(y){
ne<-ncol (y)
means<- numeric(nc)
for (i in 1:nc){
means[i]<- mean(y[,1])
}
means
}
make.power<-function(n){pow<-function(x){x^n}pow}
cube <- make.power(3)
lexical scoping.R
pow
}
cube <-make.power(3)
cube <-make.power(3)
x^3
}
{
x^3
}
cube(3)
cube(3)
}
z<-10
f(3)
f(3)
z<-10
f(3)
z<-10
f(3)
f <- function(x) {
g <- function(y) {
y + z
}
z <- 4
x + g(x)
}
z<-10 f(3)
z<-10
f(3)
library(datasets)
data(iris)
?iris
librar(datasets)
library(datasets)
data(mtcars)
?mtcars
inv<- NULL
}
x = rbind(c(1, -1/4), c(-1/4, 1))
m = makeCacheMatrix(x)
makeCacheMatrix <- function(x = matrix()) {
#store cache of inverse matrix
inv<- NULL
#1. set value of the matrix
set<-function(y){
x<<-y
invCache<<-NULL
}
#2.get value for matrix
get <- function() x
setinverse <- function(inverse) inv <<- inverse
getinverse <- function() inv
list(set=set, get=get, setinverse=setinverse, getinverse=getinverse)
}
## This function computes the inverse of the special "matrix" returned by makeCacheMatrix above.
##If the inverse has already been calculated (and the matrix has not changed), then the cachesolve
##should retrieve the inverse from the cache.
cacheSolve <- function(x, ...) {
inv <- x$getinverse()
if(!is.null(inv)) {
message("just a sec, retreiving cached data.")
return(inv)
}
data <- x$get()
## Put comments here that give an overall description of what your
## functions do
clear
install.packages("swirl")
library(swirl)
swirl()
swirl()
5 + 7
x <- 5 + 7
x
y <- x-3
y
z<-c(1.1,9, 3.14)
?
z<-c(1.1,9, 3.14)
?c
z
```
xyplot(weight ~ Time | Diet, BodyWeight)
library(httr)
require(httpuv)
require(jsonlite)
```{r}
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
f <- file.path(getwd(), "ss06pid.csv")
download.file(url, f)
acs <- data.table(read.csv(f))
query1 <- sqldf("select pwgtp1 from acs where AGEP < 50")
query2 <- sqldf("select pwgtp1 from acs")  ## NO
query3 <- sqldf("select * from acs where AGEP < 50 and pwgtp1")  ## NO
query4 <- sqldf("select * from acs where AGEP < 50")  ## NO
identical(query3, query4)
```
```
```{r}
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
f <- file.path(getwd(), "ss06pid.csv")
download.file(url, f)
acs <- data.table(read.csv(f))
query1 <- sqldf("select pwgtp1 from acs where AGEP < 50")
query2 <- sqldf("select pwgtp1 from acs")  ## NO
query3 <- sqldf("select * from acs where AGEP < 50 and pwgtp1")  ## NO
query4 <- sqldf("select * from acs where AGEP < 50")  ## NO
identical(query3, query4)
```
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
f <- file.path(getwd(), "ss06pid.csv")
download.file(url, f)
acs <- data.table(read.csv(f))
acs <- data.table(read.csv("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"))
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
f <- file.path(getwd(), "ss06pid.csv")
download.file(url, f)
oauth_endpoints("github")
myapp <- oauth_app("quiz2", "ddb0d599de51ccd02f4b", secret = "6af1109f6ecf442d292425087d49bb13d9bbe9c8")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
setwd("~/Desktop/coursera/GettingandCleaningData/getting_and_cleaning_data_project")
#Finally, we can row bind everything to one set
allTheData= rbing(training,testing)
features = read.csv("UCI HAR Dataset/features.txt", sep="", header=FALSE)
features[,2] = gsub('-mean', 'Mean', features[,2])
features[,2] = gsub('-std', 'Std', features[,2])
#set my own personal working directory for my comp
tidy = aggregate(allTheData, by=list(activity = allTheData$activity, subject=allTheData$subject), mean)
# Remove the subject and activity column, since a mean of those has no use
tidy[,90] = NULL
run_analysis.R
run run_analysis.R
write.table(tidy, "tidy.txt", sep="\t")
#set my own personal working directory for my comp
#used some code from github to fill in holes
setwd("~/Desktop/coursera/GettingandCleaningData/getting_and_cleaning_data_project")
##TASK 1: MERGE THE TRAINING AND TEST SETS TO CREATE ONE DATA SET
#Train- load the set from X, Y & subject
training = read.csv ("UCI HAR Dataset/train/X_train.txt", sep= "", header= FALSE)
training[,562] = read.csv("UCI HAR Dataset/train/Y_train.txt", sep="", header=FALSE)
training[,563] = read.csv("UCI HAR Dataset/train/subject_train.txt", sep="", header=FALSE)
#Testing- load the set from X, Y & subject
testing = read.csv ("UCI HAR Dataset/test/X_test.txt", sep= "", header= FALSE)
testing[,562] = read.csv("UCI HAR Dataset/test/Y_test.txt", sep="", header=FALSE)
testing[,563] = read.csv("UCI HAR Dataset/test/subject_test.txt", sep="", header=FALSE)
#Activities- only one file
activity = read.csv ("activity_labels.txt", sep= "", header= FALSE)
# Read features and make the feature names better suited for R with some substitutions
features = read.csv("UCI HAR Dataset/features.txt", sep="", header=FALSE)
features[,2] = gsub('-mean', 'Mean', features[,2])
features[,2] = gsub('-std', 'Std', features[,2])
features[,2] = gsub('[-()]', '', features[,2])
#Finally, we can row bind everything to one set
allTheData= rbind(training,testing)
##TASK 2.Extracts only the measurements on the mean and standard deviation for each measurement.
demCols <- grep(".*Mean|.*Std", features [,2])
features <- features[demCols,]
demCols <- c(demCols, 562, 563)
#removed unwanted data, cleaned up
allTheData <- allTheData[,demCols]
##TASK 3.Uses descriptive activity names to name the activities in the data set
colnames(allTheData) <-c(features, "Activities", "Subjects")
##TASK 4.Appropriately labels the data set with descriptive variable names.
colnames(allTheData) <- tolower(colnames(allTheData))
#this is where i got stuck mostly
currentActivity = 1
for (currentActivityLabel in activityLabels$V2) {
allTheData$activity <- gsub(currentActivity, currentActivityLabel, allTheData$activity)
currentActivity <- currentActivity + 1
}
allTheData$activity <- as.factor(allTheData$activity)
alThelData$subject <- as.factor(allTheData$subject)
##TASK 5.Creates a second, independent tidy data set with the average of each variable for each activity and each subject.
tidy = aggregate(allTheData, by=list(activity = allTheData$activity, subject=allTheData$subject), mean)
# Remove the subject and activity column, since a mean of those has no use
tidy[,90] = NULL
tidy[,89] = NULL
write.table(tidy, "tidy.txt", sep="\t")
clear
